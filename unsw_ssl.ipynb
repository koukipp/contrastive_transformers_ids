{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import cuda\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkSize = 2500000\n",
    "i = 1\n",
    "for chunk in pd.read_csv(\"unsw_1_labelled_flows.csv\", chunksize=ChunkSize):\n",
    "    #chunk.dropna(subset=['ip.src', 'ip.dst'],inplace=True)\n",
    "    #chunk.drop(['tcp.seq','tcp.ack','ip.ttl','ip.hdr_len'], axis=1, inplace=True)\n",
    "    dtypes = {'_ws.col.Time': 'float64',\n",
    "          'ip.src':'str',\n",
    "          'ip.dst':'str',\n",
    "          'ip.hdr_len':'uint32',\n",
    "          'ip.dsfield': 'uint32',\n",
    "          'ip.flags': 'uint32',\n",
    "          'ip.len': 'uint32',\n",
    "          'ip.ttl': 'uint32',\n",
    "          'ip.proto': 'uint32',\n",
    "          'packet.len': 'uint32',\n",
    "          'tcp.dstport': 'uint32',\n",
    "          'tcp.srcport': 'uint32',\n",
    "          'tcp.flags': 'uint32',\n",
    "          'tcp.seq': 'uint32',\n",
    "          'tcp.ack': 'uint32',\n",
    "          'tcp.window_size_value': 'uint32',\n",
    "          'tcp.payload':'object',\n",
    "          'tcp.label':'uint32',\n",
    "          'flow_id':'uint64',\n",
    "          'subflow': 'uint32',\n",
    "          'direction': 'int32',\n",
    "          'payload_0' : 'uint32',\n",
    "          'payload_1' : 'uint32',\n",
    "          'payload_2' : 'uint32',\n",
    "          'payload_3' : 'uint32',\n",
    "          'payload_4' : 'uint32',\n",
    "          'payload_5' : 'uint32',\n",
    "          'Attack category': 'str',\n",
    "          }\n",
    "\n",
    "    for col in chunk.columns:\n",
    "        chunk[col] = chunk[col].astype(dtypes[col])\n",
    "\n",
    "    if i == 1:\n",
    "        data = chunk\n",
    "    else:\n",
    "        data = pd.concat([data, chunk])\n",
    "    print('-->Read Chunk...', i)\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cac2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Attack category'] != 'Benign', 'Attack category'] = 1\n",
    "data.loc[data['Attack category'] == 'Benign', 'Attack category'] = 0\n",
    "data.rename(columns={'Attack category' : \"label\"}, inplace=True)\n",
    "data = data[data['ip.proto'].isin([6,17,1,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=['_ws.col.Time'],inplace=True, ignore_index = True)\n",
    "data = data.groupby('flow_id').head(512)\n",
    "data.drop_duplicates(data.columns.delete(0).delete(6).delete(13),inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f93f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.groupby('flow_id').head(32)\n",
    "\n",
    "\n",
    "def get_rel_time():\n",
    "    data['_ws.col.Time'] = data.groupby('flow_id')['_ws.col.Time'].transform(lambda x:  x - x.iloc[0])\n",
    "\n",
    "get_rel_time()\n",
    "\n",
    "data.drop(data[data['_ws.col.Time'] > 120].index,inplace=True)\n",
    "\n",
    "\n",
    "data['label'] = data.groupby('flow_id')['label'].transform('max')\n",
    "data.drop('tcp.ack', axis=1,inplace=True)\n",
    "data.drop('tcp.seq', axis=1,inplace=True)\n",
    "data.drop('ip.ttl', axis=1,inplace=True)\n",
    "\n",
    "def get_rel_time():\n",
    "    data['_ws.col.Time'] = data.groupby('flow_id')['_ws.col.Time'].diff().fillna(0)\n",
    "\n",
    "get_rel_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = {\"ip.src\", \"ip.dst\",'udp.srcport', 'tcp.srcport','Unnamed: 0', 'tcp.dstport'}\n",
    "cat_fields = ['ip.proto', 'ip.flags','tcp.flags','ip.dsfield']\n",
    "mixed_cols = {'tcp.flags','ip.flags','ip.dsfield'}\n",
    "payload_fields = ['payload_' + str(i) for i in range(6)]\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "#for col in mixed_cols:\n",
    "#    data[col] = data[col].map(lambda hexStr: int(hexStr, 16), na_action=\"ignore\")\n",
    "\n",
    "#data.dropna(axis=0, how='any', inplace=True)\n",
    "#data.drop_duplicates(subset=None, keep=\"first\", inplace=True)\n",
    "\n",
    "label_field = {'label'}\n",
    "num_fields = set(data.columns) - set(cat_fields) - label_field - {\"iat\"} -  {\"_ws.col.Time\"} - {\"flow_id\"} - {\"direction\"} - {\"subflow\"} - drop_columns - set(payload_fields) - {'group_size'} - {'group_size_pkt'}\n",
    "num_fields = sorted(num_fields)\n",
    "labels = data['label'].nunique()\n",
    "print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67198aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "DATE_FORMAT_DATASET = '%Y-%m-%d %H:%M:%S.%f'\n",
    "\n",
    "np.random.seed(52)\n",
    "torch.manual_seed(52)\n",
    "torch.cuda.manual_seed(52)\n",
    "\n",
    "\n",
    "# Select the length of the packet sequence\n",
    "seq_len = 1024\n",
    "unique_flows = data['flow_id'].unique()\n",
    "\n",
    "def get_train_test_mask(data, seq_len):\n",
    "    train_mask = np.full(data.shape[0], False)\n",
    "    test_mask = np.full(data.shape[0], False)\n",
    "    train_sample = set()\n",
    "    test_sample = set()\n",
    "    for i in data['label'].unique():\n",
    "        group = data[data['label'] == i]['flow_id'].unique()\n",
    "        eval_sample = np.random.choice(group, len(group), replace=False)\n",
    "        if i == 0:\n",
    "            train_sample |= (set(eval_sample[:round(0.6*len(eval_sample))]))\n",
    "            test_sample |= (set(eval_sample[round(0.6*len(eval_sample)):]))\n",
    "        elif i == 1:\n",
    "            test_sample |= (set(eval_sample))\n",
    "    train_mask[data.index[data['flow_id'].isin(train_sample)].to_list()] = True\n",
    "    test_mask[data.index[data['flow_id'].isin(test_sample)].to_list()] = True\n",
    "    return train_mask,test_mask,train_mask ,np.array(list(train_sample)),np.array(list(test_sample))\n",
    "\n",
    "\n",
    "def fit_numerical(new_data, num_fields, eval_mask):\n",
    "    for col in num_fields:\n",
    "        print(col)\n",
    "        col_values = data[col].values\n",
    "        col_values[~np.isfinite(col_values)] = 0\n",
    "        col_values = col_values.astype(\"uint32\")\n",
    "        new_data[col] = col_values\n",
    "\n",
    "def fit_categorical(new_data, cat_fields, eval_mask):\n",
    "    for col in cat_fields:\n",
    "        print(col)\n",
    "        pre_values = data[col].values\n",
    "        new_data[col] =  pre_values + 3\n",
    "\n",
    "        print(col,np.unique(new_data[col]))\n",
    "\n",
    "def create_new_df(num_fields, cat_fields, eval_mask, seq_len):\n",
    "    new_data = {}\n",
    "    new_data['flow_id'] = data['flow_id']\n",
    "    new_data['_ws.col.Time'] = data['_ws.col.Time']\n",
    "    new_data['direction'] = data['direction'].astype(\"int32\") + 3\n",
    "\n",
    "    print(new_data['_ws.col.Time'])\n",
    "    fit_categorical(new_data, cat_fields, eval_mask)\n",
    "    fit_numerical(new_data, num_fields, eval_mask)\n",
    "    for f in payload_fields:\n",
    "        new_data[f] = data[f].apply(lambda x: x + 1 if x > 1 else x).astype(\"int32\") \n",
    "    return pd.DataFrame(new_data)\n",
    "\n",
    "train_mask,test_mask,fit_mask,train_idx,test_idx = get_train_test_mask(data, seq_len)\n",
    "new_df = create_new_df(num_fields, cat_fields, fit_mask, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc5b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "def generate_batches(train_mask, test_mask, batch_size):\n",
    "    batches_train = np.random.choice(train_mask, (len(train_mask)//batch_size, batch_size), replace=False)\n",
    "    batches_test = np.random.choice(test_mask, (len(test_mask)//128, 128), replace=False)\n",
    "    return batches_train, batches_test\n",
    "\n",
    "batches_train,batches_test = generate_batches(train_idx,test_idx,batch_size)\n",
    "new_df = new_df.join(data['label'])\n",
    "new_df['count'] = new_df['flow_id'].map(new_df['flow_id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d19716",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop('payload_0', axis=1,inplace=True)\n",
    "new_df.drop('payload_1', axis=1,inplace=True)\n",
    "new_df.drop('payload_2', axis=1,inplace=True)\n",
    "new_df.drop('payload_3', axis=1,inplace=True)\n",
    "new_df.drop('payload_4', axis=1,inplace=True)\n",
    "new_df.drop('payload_5', axis=1,inplace=True)\n",
    "new_df['_ws.col.Time'] = new_df['_ws.col.Time'] \n",
    "new_df['packet.len'] = new_df['packet.len'] / 1400\n",
    "new_df['_ws.col.Time'] = new_df['_ws.col.Time'] * 10 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6845af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler,MinMaxScaler,StandardScaler\n",
    "new_df.index.name = 'index'\n",
    "new_df.sort_values(by=['flow_id', 'index'],inplace=True)\n",
    "new_df.set_index('flow_id', inplace=True)\n",
    "new_df.drop('ip.len', axis=1,inplace=True)\n",
    "new_df.drop('ip.dsfield', axis=1,inplace=True)\n",
    "new_df.drop('tcp.window_size_value', axis=1,inplace=True)\n",
    "new_df.drop('ip.flags', axis=1,inplace=True)\n",
    "new_df.drop('ip.hdr_len', axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a266d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SLICING\n",
    "train_df = new_df[new_df.index.isin(train_idx)]\n",
    "test_df = new_df[new_df.index.isin(test_idx)]\n",
    "\n",
    "counts_indexes = {}\n",
    "for i in range(1,33):\n",
    "    counts_indexes[i] = train_df[train_df['count'] == i].index\n",
    "\n",
    "train_df.drop('count', axis=1,inplace=True)\n",
    "test_df.drop('count', axis=1,inplace=True)\n",
    "new_df.drop('count', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc4ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "labels = 1\n",
    "## SLICING\n",
    "def batch_loader(batches, device, new_df, mask = True):\n",
    "    matching_rows_list = []\n",
    "    label_rows_list = []\n",
    "    matching_rows_list_pos = []\n",
    "    mask_rows = []\n",
    "    mask_rows_2 = []\n",
    "    sliced_df = new_df[new_df.index.isin(batches)]\n",
    "    unique_combinations = sliced_df.groupby('flow_id')\n",
    "\n",
    "    # Iterate over the groups\n",
    "    for group_name, group in unique_combinations:\n",
    "        seq_x = group.to_numpy()[:32,:-labels]\n",
    "        if mask:\n",
    "            seq_x_1 = seq_x.copy()\n",
    "\n",
    "            \n",
    "\n",
    "            matching_rows_list.append(torch.tensor(seq_x_1.astype('float32'), device=device))\n",
    "            mask_rows.append(torch.ones(seq_x_1.shape[0] + 1, device=device))\n",
    "\n",
    "            seq_x_2 = seq_x.copy()\n",
    "            if seq_x.shape[0] > 1:\n",
    "                new_seq = new_df.loc[np.random.choice(counts_indexes[seq_x.shape[0]])].to_numpy()\n",
    "                if new_seq.ndim == 1:\n",
    "                    new_seq = new_seq.reshape(1, -1)\n",
    "                new_seq = new_seq[:32,:-labels]\n",
    "\n",
    "                # Choose a random cut length (avoiding full replacement)\n",
    "                cut_length = round(0.4 * (seq_x.shape[0]))\n",
    "                # Choose a random starting position in both embeddings\n",
    "                start_a = np.random.randint(0, seq_x.shape[0] - cut_length + 1)\n",
    "\n",
    "                \n",
    "                \n",
    "                # Create the new mixed embedding by replacing a segment from emb_a with emb_b\n",
    "                seq_x_2[start_a:start_a + cut_length] = new_seq[start_a:start_a + cut_length]\n",
    "            \n",
    "            \n",
    "            matching_rows_list_pos.append(torch.tensor(seq_x_2.astype('float32'), device=device))\n",
    "            mask_rows_2.append(torch.ones(seq_x_2.shape[0] + 1, device=device))\n",
    "        else:\n",
    "\n",
    "            matching_rows_list.append(torch.tensor(seq_x.astype('float32'), device=device))\n",
    "            label_rows_list.append(torch.tensor(group.to_numpy()[0,-labels:].astype('float32'), device=device))\n",
    "            mask_rows.append(torch.ones(seq_x.shape[0] + 1, device=device))\n",
    "\n",
    "\n",
    "\n",
    "    tensor_x = torch.nn.utils.rnn.pad_sequence(matching_rows_list, batch_first=True, padding_value=0)\n",
    "    cls_token =  torch.cat((torch.zeros(tensor_x.shape[0], 1, 1, device=device, dtype=torch.float), torch.ones(tensor_x.shape[0], 1, 4, device=device, dtype=torch.float)), 2)\n",
    "    tensor_x = torch.cat((cls_token,tensor_x), 1)\n",
    "    tensor_x[:,1,0] =  0\n",
    "    tensor_x[:,0,4] =  0\n",
    "\n",
    "    if mask:\n",
    "        tensor_y = torch.nn.utils.rnn.pad_sequence(matching_rows_list_pos, batch_first=True, padding_value=0)\n",
    "        cls_token =  torch.cat((torch.zeros(tensor_y.shape[0], 1, 1, device=device, dtype=torch.float), torch.ones(tensor_y.shape[0], 1, 4, device=device, dtype=torch.float)), 2)\n",
    "        tensor_y = torch.cat((cls_token,tensor_y), 1)\n",
    "        tensor_y[:,1,0] =  0\n",
    "        tensor_y[:,0,4] =  0\n",
    "\n",
    "        masked = torch.nn.utils.rnn.pad_sequence(mask_rows, batch_first=True, padding_value=0)\n",
    "        masked_2 = torch.nn.utils.rnn.pad_sequence(mask_rows_2, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "        tensor_y = torch.nn.utils.rnn.pad_sequence(label_rows_list, batch_first=True, padding_value=0)\n",
    "        masked = torch.nn.utils.rnn.pad_sequence(mask_rows, batch_first=True, padding_value=0)\n",
    "        masked_2 = masked\n",
    "    \n",
    "    masked = ~masked.to(torch.bool)\n",
    "    masked_2 = ~masked_2.to(torch.bool)\n",
    "    #print(count)\n",
    "\n",
    "    return tensor_x,tensor_y,masked,masked_2\n",
    "\n",
    "#diff_avg_all = []\n",
    "for i, batch in enumerate(batches_train):\n",
    "    Xbatch,ybatch,mask,mask_2 = batch_loader(batch, device, train_df)\n",
    "    print(Xbatch.shape)\n",
    "    print(Xbatch[0,:,0])\n",
    "    print(ybatch[0,:,0])\n",
    "    if i == 1:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "\n",
    "class EmbeddingLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super(EmbeddingLayer, self).__init__()\n",
    "      self.position_embeddings = torch.nn.Embedding(34, hidden_dim)\n",
    "      self.embedding_layer = torch.nn.ModuleList([torch.nn.Embedding(65539, 51) for col in new_df.columns[1:-(labels + 1)]])\n",
    "      self.time_layer = torch.nn.Linear(1,51,bias=False)\n",
    "      self.time_layer_2 = torch.nn.Linear(128,51)\n",
    "\n",
    "      self.packet_layer = torch.nn.Linear(1,51,bias=False)\n",
    "      self.packet_layer_2 = torch.nn.Linear(128,51)\n",
    "\n",
    "      self.proj_layer = torch.nn.Linear(51 * (new_df.shape[-1]  - labels), hidden_dim)\n",
    "      torch.nn.init.xavier_uniform_(self.proj_layer.weight)\n",
    "      self.proj_layer.bias.data.fill_(0)\n",
    "      self.activ = torch.nn.ReLU()\n",
    "      self.norm_3 = torch.nn.LayerNorm(256)\n",
    "      self.norm_2 = torch.nn.LayerNorm(45 * (new_df.shape[-1]  - labels - 1))\n",
    "      self.norm_1 = torch.nn.LayerNorm(273,eps=1e-12)\n",
    "      self.dropout = torch.nn.Dropout(0.1)\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        position = torch.arange(input.shape[1], dtype=torch.long, device=device)\n",
    "        position = position.unsqueeze(0).expand((input.shape[0],input.shape[1]))\n",
    "\n",
    "\n",
    "        list_emb = self.time_layer(input[:, :, [0]])\n",
    "        packet_emb = self.packet_layer(input[:, :, [4]])\n",
    "        list_emb_2 = [self.embedding_layer[i-1](input[:, :, i].int()) for i in range(1,input.shape[-1] - 1)]\n",
    "\n",
    "        embed_tokens = self.proj_layer(torch.cat((list_emb,packet_emb,torch.cat(list_emb_2,dim=2)),dim=2))\n",
    "        hidden_states = embed_tokens + self.position_embeddings(position)\n",
    "        return hidden_states\n",
    "  \n",
    "\n",
    "class OutputLayer(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(OutputLayer, self).__init__()\n",
    "    self.activ = torch.nn.ReLU()\n",
    "    self.linear =  torch.nn.Linear(hidden_dim,hidden_dim)\n",
    "    self.linear_2 =  torch.nn.Linear(hidden_dim,hidden_dim)\n",
    "  \n",
    "  def forward(self, input):\n",
    "      cls = self.linear_2(self.activ(self.linear(input)))\n",
    "      return cls\n",
    "\n",
    "class CLSLayer(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CLSLayer, self).__init__()\n",
    "    self.activ = torch.nn.ReLU()\n",
    "    self.sig_activ = torch.nn.Sigmoid()\n",
    "    self.linear =  torch.nn.Linear(hidden_dim,4*hidden_dim)\n",
    "    self.linear_2 =  torch.nn.Linear(4*hidden_dim,hidden_dim)\n",
    "    self.linear_3 =  torch.nn.Linear(hidden_dim,1)\n",
    "  \n",
    "  def forward(self, input):\n",
    "      cls = self.sig_activ(self.linear_3(self.linear_2(self.activ(self.linear(input)))))\n",
    "      return cls\n",
    "  \n",
    "\n",
    "class BERT(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BERT, self).__init__()\n",
    "    self.norm = torch.nn.LayerNorm(768)\n",
    "    self.embed = EmbeddingLayer()\n",
    "    self.encoder_layer = torch.nn.TransformerEncoderLayer(hidden_dim, 4, 256*4, batch_first=True, activation = 'gelu')\n",
    "    self.encoder = torch.nn.TransformerEncoder(self.encoder_layer, num_layers=4)\n",
    "    self.out_layer = OutputLayer()\n",
    "    self.cls_layer = CLSLayer()\n",
    "  \n",
    "  def forward(self, input_1, input_2, mask):\n",
    "      embed_1 = self.embed(input_1)\n",
    "      enc = self.encoder(embed_1,src_key_padding_mask=mask)\n",
    "      cls_1 = self.out_layer(enc[:, 0, :])\n",
    "      \n",
    "      embed_2 = self.embed(input_2) \n",
    "      enc = self.encoder(embed_2,src_key_padding_mask=mask)\n",
    "      cls_2 = self.out_layer(enc[:, 0, :])\n",
    "      return cls_1,cls_2\n",
    "  \n",
    "  def embeddings(self, input, mask):\n",
    "      embed = self.embed(input)\n",
    "      enc = self.encoder(embed,src_key_padding_mask=mask)\n",
    "      return enc[:, 0, :]\n",
    "\n",
    "  def embeddings_cls(self, input, mask):\n",
    "      embed = self.embed(input)\n",
    "      enc = self.encoder(embed,src_key_padding_mask=mask)\n",
    "      return self.cls_layer(enc[:, 0, :])\n",
    "\n",
    "\n",
    "class CosineWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor\n",
    "        \n",
    "model = BERT()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class NTXent(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        \"\"\"NT-Xent loss for contrastive learning using cosine distance as similarity metric as used in [SimCLR](https://arxiv.org/abs/2002.05709).\n",
    "        Implementation adapted from https://theaisummer.com/simclr/#simclr-loss-implementation\n",
    "\n",
    "        Args:\n",
    "            temperature (float, optional): scaling factor of the similarity metric. Defaults to 1.0.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"Compute NT-Xent loss using only anchor and positive batches of samples. Negative samples are the 2*(N-1) samples in the batch\n",
    "\n",
    "        Args:\n",
    "            z_i (torch.tensor): anchor batch of samples\n",
    "            z_j (torch.tensor): positive batch of samples\n",
    "\n",
    "        Returns:\n",
    "            float: loss\n",
    "        \"\"\"\n",
    "        batch_size = z_i.size(0)\n",
    "        #z_i = F.normalize(z_i, p=2, dim=1)\n",
    "        #z_j = F.normalize(z_j, p=2, dim=1)\n",
    "\n",
    "        # compute similarity between the sample's embedding and its corrupted view\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        similarity = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)\n",
    "\n",
    "        sim_ij = torch.diag(similarity, batch_size)\n",
    "        sim_ji = torch.diag(similarity, -batch_size)\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)  # size is 2*bs\n",
    "\n",
    "        mask = (\n",
    "            ~torch.eye(batch_size * 2, batch_size * 2, dtype=torch.bool)\n",
    "        ).float().to(device)\n",
    "        numerator = torch.exp(positives / self.temperature)\n",
    "        denominator = mask * torch.exp(similarity / self.temperature)\n",
    "\n",
    "        all_losses = -torch.log(numerator / torch.sum(denominator, dim=1))\n",
    "        loss = torch.sum(all_losses) / (2 * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450472ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def model_accuracy(model, device, y_pred = None):\n",
    "    accuracy = 0.0\n",
    "    running_vloss = 0.0\n",
    "    inf_time = 0.0\n",
    "    y_label = []\n",
    "    y_test = None\n",
    "    cos_sim_out = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if y_pred == None:\n",
    "            for i, batch in enumerate(batches_train):\n",
    "                tensor_x_test, tensor_y_test, mask, mask_2 = batch_loader(batch, device, train_df, mask= False)\n",
    "                \n",
    "                # Measure inference time for GPU\n",
    "                if device == 'cuda':\n",
    "                    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "                    starter.record()\n",
    "                    y_out = model.embeddings(tensor_x_test,mask).clone()\n",
    "                    ender.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    inf_time += starter.elapsed_time(ender)\n",
    "                else: # Or for CPU\n",
    "                    start = time.time()\n",
    "                    y_out = model(tensor_x_test)\n",
    "                    end = time.time() - start\n",
    "                    inf_time += end * 1000\n",
    "                    \n",
    "                if y_pred == None:\n",
    "                    y_pred = y_out\n",
    "                else:\n",
    "                    y_pred = torch.cat([y_pred,y_out])\n",
    "\n",
    "\n",
    "            y_pred = F.normalize(y_pred, p=2, dim=1)\n",
    "\n",
    "        cos_sim_lst = []\n",
    "        roc_score = 0\n",
    "\n",
    "        for i, batch in enumerate(batches_test):\n",
    "            tensor_x_test, tensor_y_test, mask, mask_2 = batch_loader(batch, device, test_df, mask= False)\n",
    "            \n",
    "            # Measure inference time for GPU\n",
    "            if device == 'cuda':\n",
    "                starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "                starter.record()\n",
    "                y_out = F.normalize(model.embeddings(tensor_x_test,mask).clone(), p=2, dim=1)\n",
    "                similarity = torch.mm(y_out, y_pred.t())\n",
    "                cos_sim,indices = similarity.max(dim=1)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize()\n",
    "                inf_time += starter.elapsed_time(ender)\n",
    "                cos_sim_lst.append(cos_sim.cpu())\n",
    "            else: # Or for CPU\n",
    "                start = time.time()\n",
    "                y_out = model(tensor_x_test)\n",
    "                end = time.time() - start\n",
    "                inf_time += end * 1000\n",
    "            \n",
    "            if i == 0:\n",
    "                for j in range(cos_sim.shape[0]):\n",
    "                    print(j,indices[j],float(cos_sim[j]),int(1 - tensor_y_test[j]))\n",
    "            \n",
    "            if y_test == None:\n",
    "                y_test = tensor_y_test.cpu()\n",
    "                cos_sim_out = cos_sim.cpu()\n",
    "            else:\n",
    "                y_test = torch.cat([y_test,tensor_y_test.cpu()])\n",
    "                cos_sim_out = torch.cat([cos_sim_out,cos_sim.cpu()])\n",
    "            \n",
    "\n",
    "        roc_score = roc_auc_score(y_test, 1 - cos_sim_out)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, 1 - cos_sim_out)\n",
    "        # Find the best threshold\n",
    "        # Calculate Youden's J statistic\n",
    "        youden_j = tpr - fpr\n",
    "        best_threshold_index = np.argmax(youden_j)\n",
    "        best_threshold = thresholds[best_threshold_index]\n",
    "        print(accuracy_score(y_test, (1 - cos_sim_out) >= best_threshold))\n",
    "        print(classification_report(y_test, (1 - cos_sim_out) >= best_threshold,digits=4))\n",
    "\n",
    "        print(\"Best Threshold:\", best_threshold)\n",
    "        print(\"TPR:\", tpr[best_threshold_index], \"FPR:\", fpr[best_threshold_index])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    print(\"ROC_AUC_SCORE\",roc_score)\n",
    "    return roc_score,y_pred\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, epochs, lr, batches_train, device):\n",
    "    loss_fn = NTXent()\n",
    "    train_time = 0.0\n",
    "    batch_arr = batches_train.copy()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.\n",
    "        batch_loss = 0.\n",
    "        model.train(True)\n",
    "        for i, batch in enumerate(batch_arr):\n",
    "            if device == 'cuda':\n",
    "                starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "                starter.record()\n",
    "                loss = train_batch(model, loss_fn, optimizer, batch)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize()\n",
    "                train_time += starter.elapsed_time(ender)\n",
    "            else:\n",
    "                start = time.time()\n",
    "                loss = train_batch(model, loss_fn, optimizer, Xbatch, ybatch)\n",
    "                end = time.time() - start\n",
    "                train_time += end * 1000    \n",
    "\n",
    "            running_loss += loss.item()\n",
    "            batch_loss += loss.item()\n",
    "            if i % 10 == 0:\n",
    "                print(\"batch \", i, batches_train.shape, loss.item(), batch_loss / 10)\n",
    "                batch_loss = 0.\n",
    "            \n",
    "\n",
    "        last_loss = running_loss / (i+1)\n",
    "        print('epoch {} batches {} loss: {} time: {} ms'.format(epoch, i + 1, last_loss, train_time))\n",
    "        batch_arr = batch_arr.reshape(-1,1)\n",
    "        np.random.shuffle(batch_arr)\n",
    "        batch_arr = batch_arr.reshape(batches_train.shape[0],batches_train.shape[1])\n",
    "        model.eval()\n",
    "        roc_score,y_pred = model_accuracy(model, device='cuda')\n",
    "    return roc_score,y_pred\n",
    "        \n",
    "def train_batch(model, loss_fn, optimizer, batch):\n",
    "    optimizer.zero_grad()\n",
    "    Xbatch, xbatch_pair ,att_mask ,att_mask_2 = batch_loader(batch, device, train_df, mask= True)\n",
    "    anchor,pair = model(Xbatch, xbatch_pair,att_mask)\n",
    "    loss = loss_fn(anchor, pair)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #scheduler.step()  # Update the learning rate\n",
    "    return loss\n",
    "\n",
    "roc_score,y_pred = train_model(model, epochs=1, lr=0.0001, batches_train=batches_train, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
